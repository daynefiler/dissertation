# Introduction {#intro}

## Outline

## Human genetics primer

### Discovery of DNA and the central dogma

The discovery of deoxyribonucleic acid (DNA) took roughly 100 years of work, and has fundamentally changed how we view ourselves, society, and life.
The study of genetics begins with the study of peas and the discovery of inheritance by Gregor Mendel in the middle of the 19th century [@mendel:1866aa].
Shortly after Mendel's work, Friedrich Miescher isolated "nuclein" from lymphocytes noting the uniquenely high proportion of phosphorus in the form of phosphoric acid [@miescher:1871aa; @miescher:1874aa; @miescher:1874ab]. 
Albrecht Kossel and Albert Neumann furthered Miescher's work by identifying the four bases and renaming "nuclein" deoxyribonucleic acid [@kossel:1893aa]. 
Walther Flemming first described mitosis (the division of cells) showing the doubling and separation of chromosomes [@flemming:1878aa].
Theodor Boveri and Walter Sutton independently discovered meiosis, establishing chromosomes as the vehicle for inheritance (i.e. the "chromosome theory of inheritance") [@boveri:1902aa; @boveri:1903aa; @sutton:1903aa].

Despite early suggestions of chromatin containing DNA by Kossel and Neumann, many believed proteins and not DNA coded the fundamental information for inheritance.
Oswald Avery, Collin MacLeod, and Maclyn McCarty published the first experiments to establish DNA carries the hedidary code using *Diplococcus pneumoniae* [@avery:1944aa].
Erwin Chargaff rightly believed the work by Avery *et al.* and went on to discover equal proportions of adenine/thymine guanine/cytosine ("Chargaff's rule") which disproved the tetranucleotide hypothesis and laid the groundwork for the double helical model [@chargaff:1949aa]. 
In the early 1950's Roslind Franklin started using X-ray crystallography to study the structure of DNA, producing the first images showing the double helical form [@franklin:1953aa].
Watson and Crick were given Rosalind's images without her knowledge or permission, allowing them to perform the final work to establish the structure of DNA [@watson:1953aa].
Crick went on to establish the Central Dogma of Molecular Biology [@crick:1958aa; @crick:1970aa].

The Central Dogma of Molecular Biology describes the process by which DNA codes for the proteins that build and sustain eukaryotic life.
To produce proteins, ribonucleic acid (RNA) polymerase first transcribes the DNA message into single-stranded RNA molecules (messanger RNA, mRNA).
The mRNA, after post-transcriptional modifications including possible splicing (reorganization), is then translated into a polymer of amino acids by ribosomal RNA complexes.
Amino acid polymers, also known as polypeptides or peptide chains, form the primary structure of proteins.
Therefore, modifications to DNA have profound impacts on cellular and organismal function. 

Sequencing an individual reveals millions of variants compared to the current reference genome [@auton:2015aa], often requiring filtering to identify meaning results.
In identifying pathogenic (disease-causing) variants, 

### Types of genetic variation

Humans are diploid organisms, meaning we have two copies of each chromosome.
Under normal circumstances, we receive one set of chromosomes each from our biological mother and father.
Humans have 46 chromosomes (23 from each parent), including 21 autosomes (chromosomes 1-22) and two sex chromosomes (X and Y).

## DNA Sequencing

### First-generation sequencing

Using lessons learned from previous RNA sequencing efforts, the first DNA sequencing techniques arose in the 1970s with Sanger's original plus-minus approach [@sanger:1975aa], the Maxam-Gilbert chemical cleavage approach [@maxam:1977aa], and Sanger's chain termination approach [@sanger:1977aa].

Maxam-Gilbert sequencing works by cleaving DNA sequences at specific base pairs using specific chemical reactions.
Before cleaving, radioactive phosphorus is incorporated into the 5 prime terminus of the DNA fragment to be sequenced. 
The fragment is then cleaved randomly in four separate reactions: at either G, G and A, C, or C and T. 
The cleaved radio-labeled fragments from each of the four reactions are then size-separated and visualized on a polyacrylamide gel.

Sanger sequencing (chain termination) was the first sequencing by synthesis (SBS) approach.
Similar to Maxam-Gilbert sequencing, the target DNA fragment is replicated by the polymerase chain reaction (PCR) in four separate conditions. 
Each condition contains an equimolar mix of the four deoxynucleotide triphosphate (dNTP, DNA bases) molecules and a small amount of a single radio- or fluorescently-labeled dideoxynucleotide (ddNTP). 
The PCR reaction cannot proceed after the incorporation of a ddNTP, so each of the four reactions will contain synthesized fragments that stop at the same base.
Again, the four reactions are size-separated and visualized on a polyacrylamide gel.

### Second-generation sequencing

In the following decade Nyrèn and Lundin discovered an enzymatic method for detecting the incorporation of a new base during sequencing [@nyren:1985aa].
Pyrophosphate is released when dNTPs are incorporated into a DNA polymer; Nyrèn added two enzymes to the synethesis reaction: (1) ATP sulfurylase, which converts pyrophosphate into ATP; (2) luciferase, which converts ATP molecules into light.
After fixing the DNA template to a solid phase, sequencing is performed by watching for light reactions after adding a single base at a time.
Pyrosequencing struggles with sequencing over homopolymers (contiguous runs of the same base), with poor performance after 4-5 identical bases [@ronaghi:1998aa].

The next significant breakthrough came in the early 2000s when Li *et al.* developed the first photocleavable fluorescent nucleotide [@li:2003aa].
The novel nucleotides use a fluorescent tag to block the 3 prime hydroxyl group, which can be cleaved using a specific wavelength of light. 
This allows for SBS with a "reversible termination" of synthesis after each base incorporation.
The reversible terminators, in conjunction with the development of glass-bound colony expansion [@fedurco:2006aa], laid the groundwork for the Solexa system (acquired by Illumina) which currently dominates the sequencing field [@turcatti:2008aa].

Illumina sequencing works by creating clusters of identical DNA fragments bound to a glass plate ("flow cell"), then performing SBS using fluorescent reversible terminators.
To perform Illumina sequencing, specific sequencing adapters are ligated onto short DNA fragments to: (1) bind DNA fragments to the flow cell; (2) initiate amplification; (3) optionally identify the fragment source.
The flow cell contains a "lawn" of two short oligos bound to the glass surface; the fragments have homology to either the forward or reverse adapter.
The sequencing library containing the ligated forward and reverse adapters are added to the flow cell, where they hybridize to the lawn.
Once bound, polymerase is added and the bound oligo is extended using the hybridized DNA fragment as a template.
The original template is then washed away, leaving complementary sequences bound to the flow cell.
The free adapter then folds over to hybridize to its complement oligo, forming a bridge, and polymerase fills in the oligo to form a double-stranded fragment (bridge amplification).
The double-stranded fragment is denatured, leaving two single stranded fragments bound to the flow cell. 
Bridge amplification is repeated until each cluster contains hundreds of the same the fragment. 
The reverse fragments are then cleaved from the flow cell, and the clusters are sequenced by detecting the incorporation of fluorescent reversible terminators.
Each cluster is tracked as basepairs are incorporated, giving the final DNA sequence.

### Processing short-read sequencing data

With the advancements in sequencing chemistry, we now have the ability to sequence great amounts of DNA cheaply.
However, the massively parallel sequencing modalities only sequence small fragments of DNA (typically 50 to 500 basepairs in length) often using a "shotgun" approach -- "shotgun" referring to sequencing a randomly fragmented sample rather than a known locus.
Therefore, the nature of short-read shotgun sequencing requires robust computational approaches to process and contextualize sequence data for millions of DNA fragments.

Here, I will give an overview of processing sequencing data for a species with an established reference genome. 
Processing short-read sequencing data follows the following general steps:

1. pre-processing to remove artificially added sequence (sequencing adapters, sample barcodes, etc.) and create FASTA/FASTQ [@pearson:1988aa; @cock:2010aa] output;

2. map individual reads to their original location in the reference genome and create Sequence Alignment Map (SAM/BAM) [@li:2009aa] output;

3. optional post-mapping quality control;

4. variant identification;

5. variant filtering and interpretation.

The pre-processing step depends entirely on the sequencing chemistry and machinery uesd.
Illumina sequencers produce binary base call (BCL) files containing all of the raw base call and quality information from the sequencing run.
BCL files contain the adapter sequence (including sample barcode sequence and molecular index sequence when used in the library generation), which must be removed prior to mapping.
Due to the capacity of modern sequencing machines, most often each lane of the flow cell will contain multiple samples. 
By convention, reads from each sample are separated into individual FASTQ files.
Separating reads by sample must occur prior to discarding the adapater sequence information.
Illumina currently provides the `bcl2fastq` command line tool for performing all of the requisite tasks to produce sample-specific FASTQ files with molecular index information when applicable. 

The process of mapping individual reads (query sequences) to a reference sequence requires (1) finding the correct starting point in the reference sequence, and (2) accounting for substitutions, insertions, and deletions in the query sequence.
Smith and Waterman published the first algorithm meeting both requirements, using dynamic programming on a substitution matrix [@smith:1981aa] based on the inital work of Needleman and Wunsch [@needleman:1970aa].
The Smith-Waterman algorithm requires user-defined scores for matches, mismatches, and gaps (insertions/deletions); the algorithm will find the best possible match with the given scoring system, but requires $O(mn)$ compute time where $m$ and $n$ represent the length of the reference and query sequences.

To reduce the complexity of the problem, Altschul *et al.* developed the basic local alignment search tool (BLAST) [@altschul:1990aa].
BLAST works by breaking the query sequences into a hash table of all possible $k$-mer sub-sequences and searching the reference sequence for non-gap matches above some threshold.
For pairs of matches, BLAST extends the sequence to refine the candidate pool, and then finalizes the best candidates using the Smith-Waterman algorithm. 
Many other tools take similar hash table approaches, including hashing the reference sequence rather than the query sequence [@li:2010aa]. 

Modern alignment algorithms have futher improved efficiency by exploiting the Burrows-Wheeler Transform (BWT) [@burrows:1994aa; @li:2010aa].
The BWT creates a quickly search-able compressed representation of the reference sequence (roughly 1 gigabyte for the complete human genome), which search algorithms can hold in memory for even greater search efficiency [@lam:2008aa].
The various BWT-based algorithms differ primarilly on how they handle mismatches [@li:2010aa]
For DNA sequencing, the Burrows Wheeler alignment tool (BWA) developed and subsequently refined by Li and Durbin in 2009 remains the *de facto* industry standard [@li:2009aa; @li:2010ab].

Post-alignment processing prepares the mapped reads for variant calling.
Artificial duplicate reads can create bias in downstream variant calling, and deserve careful consideration.
Tw types of artificial duplicates can occur with Illumina sequencing: (1) PCR duplicates, (2) technical (optical and cluster) duplicates.
In large randomly-fragmented libraries sequenced to moderate depth, duplicate reads are much more likely to represent artificial than true duplicates.
Virtually all sequencing library preparation protocols include PCR amplification, producing artificial duplicate reads. 
Using non-paterned flow cells, the image processing software my incorrectly identify large/odly shaped clusters as two separate clusters.
With patterned flow cells, occasionally the same template can "jump" into an adjascent cluster.

In deeply-sequenced libraries with low complexity, we are more likely to observe true read duplicates.
Without including unique molecular identifiers (UMIs) in the adapter sequence, we have no way of distinguishing true versus artificial duplicate reads. 
A UMI is a short (generally 6-12 basepairs) sequence of random bases; all PCR duplicates will contain the same UMI sequence.
The exceedingly low probability of two true read duplicates having the same UMI allows properly controlling for artificial duplicates without removing true duplicates.

In addition to removing duplicate reads, the GATK best practices pipeline suggests adjusting the base quality scores prior to variant calling [@mckenna:2010aa; @depristo:2011aa]. 
GATK provides the BaseQualityScoreRecalibration tool, which uses machine learning models to correct for known systematic errors in sequencing.

With the final set of aligned reads, we move to identifying deviations (variants) from the reference sequence.
Numerous tools exist to perform variant calling; I will discuss the general approaches to calling the different types of variants, highlighting commonly-used algorithms.

Calling single base substitutions -- single nucleotide variants (SNVs) -- relies fundamentally on counting alleles at each locus.
At minimum, the statistical models incorporate the quality of each base call and assumptions about sequencing error rates, e.g. the samtools mpileup/bcftools call programs [@li:2011aa].
GATK previously provided a similar tool, implementing a simple Bayesian genotype likelihood model [@mckenna:2010aa; @depristo:2011aa; @van-der-auwera:2013aa], but has moved currently to a haplotype-based calling algorithm (HaplotypeCaller) [@poplin:2018aa]. 
HaplotypeCaller works by (1) identifying "active" regions containing plausible variants, (2) building possible haplotypes in the active regions using de Bruijn-like graphs, (3) assigning haplotype likelihoods to reads, and (4) calculating genotype likelihoods incorporating the estimated haplotype information.
The idea for using haplotype estimates in genotype calling originated with the freebayes algorithm [@garrison:2012aa].
The above tools all use very similar approaches to call small insertions and deletions (indels).
Development continues actively in SNV/indel variant identification, and performance between algorithms predictably differs with condition [@chen:2019aa; @xu:2018aa].

Calling larger structural variation from short-read sequencing poses greater difficulty.
SNVs and indels exist within single reads; therefore, we can view and count them directly.
We cannot directly view variation which spans lengths greater than our read (or read pair) length.
To identify larger variation, calling algorithms attempt to identify some combination of the following tw signals: (1) relative changes in sequencing depth (read depth); (2) paired read insert size and orientation (paired end mapping).

Read-depth methods, e.g.  CNVnator [@abyzov:2011aa], work by building statistical models utilizing the relative sequencing depth across the genome.
The depth bias introduced by the capture step in targeted sequencing necessitates comparing to a set of control samples, e.g. ExomeDepth [@plagnol:2012aa], rather than calculating the relative depth across the genome.
Paired-end mapping methods identify sets of reads with insert sizes outside a specified range, indicating insertions or deletions, and reads with the incorrect orientation suggesting genomic rearrangements [@korbel:2007aa].
The Lumpy algorithm [@layer:2014aa] utilizes both the read depth and paired end mapping approaches for greater detection sensitivity.
The ERDS algorithm [@zhu:2012aa] combines read depth information with allele ratios when possible.



## Medical genetics primer

1902 – Mendel's theories were finally associated with a human disease by Sir Archibald Edward Garrod, who published the first findings from a study on recessive inheritance in human beings in 1902. Garrod opened the door for our understanding of genetic disorders resulting from errors in chemical pathways in the body.

Late 1940s – Barbara McClintock discovered the mobility of genes, ultimately challenging virtually everything that was once thought to be. Her discovery of the “jumping gene,” or the idea that genes can move on a chromosome, earned her the Nobel Prize in Physiology.